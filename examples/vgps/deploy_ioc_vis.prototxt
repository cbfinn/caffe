name: "JointAndImageStateRegressionNet"
layer {
  # Input is NxTxF
  name: "forward_cost"
  type: "MemoryData"
  top: "cost_features"
  memory_data_param {
    input_shapes {
      dim: 1
      dim: 25  # T
      dim: 96  # 32joint state+64fp
      dim: 1
    }
  }
  include: {phase:TEST}
}
layer {
  # Input is NxTxF
  name: "state_data"
  type: "MemoryData"
  top: "all_states"
  memory_data_param {
    input_shapes {
      dim: 25 # T
      dim: 32  # 32joint state+64fp
      dim: 1
      dim: 1
    }
  }
  include: {phase:FORWARDA}
  include: { phase: FORWARDB}
}
layer {
  name: "image_data"
  type: "HDF5Data"
  top: "sample_frames"
  hdf5_data_param {
    source: "img_database_temp/sample_ioc_images.txt"
    batch_size: 25 # T
  }
  # batch_size: 100 # not sure where is better.
  include: {phase: FORWARDA}
  include: { phase: FORWARDB}
}
layer {
  # Input is NxTxF
  name: "data_in_demos"
  type: "MemoryData"
  top: "demo_cost_features"
  top: "demo_logis"
  memory_data_param {
    input_shapes {
      dim: 5  # batch size, must be same as slice point
      dim: 25  # T
      dim: 96  # 64+32, dimension of phi
      dim: 1
    }
    input_shapes {
      dim: 5  # batch size, must be same as slice point
      dim: 1
      dim: 1
      dim: 1
    }
  }
  include: {phase: TRAIN}
}
layer {
  name: "data_in_samples"
  type: "MemoryData"
  top: "sample_cost_features"
  top: "sample_logis"
  memory_data_param {
    input_shapes {
      dim: 5  # batch size, must be same as slice point
      dim: 25  # T
      dim: 96  # 64+32, dimension of phi
      dim: 1
    }
    input_shapes {
      dim: 5  # batch size, must be same as slice point
      dim: 1
      dim: 1
      dim: 1
     }
  }
  include: {phase: TRAIN}
}
layer {
  name: "concat_states"
  type: "Concat"
  bottom: "demo_cost_features"
  bottom: "sample_cost_features"
  top: "cost_features"
  concat_param {
    axis: 0
  }
  include: { phase: TRAIN }
}

## START: FORWARDA NETWORK ##
# Image Processing Layers
layer {
  name: "conv1/7x7_s2"
  type: "Convolution"
  bottom: "sample_frames"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  include: { phase: FORWARDA}
  include: { phase: FORWARDB}
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1_bn"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.5
  }
  include: { phase: FORWARDA}
  include: { phase: FORWARDB}
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1_bn"
  top: "conv1_bn"
  include: { phase: FORWARDA}
  include: { phase: FORWARDB}
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_bn"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  include: { phase: FORWARDA}
  include: { phase: FORWARDB}
}

layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2_bn"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.5
  }
  include: { phase: FORWARDA}
  include: { phase: FORWARDB}
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2_bn"
  top: "conv2_bn"
  include: { phase: FORWARDA}
  include: { phase: FORWARDB}
}

layer {
  name: "conv3"
  type: "Convolution"
  bottom: "conv2_bn"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  include: { phase: FORWARDA}
  include: { phase: FORWARDB}
}

layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3_bn"
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.5
  }
  include: { phase: FORWARDA}
  include: { phase: FORWARDB}
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3_bn"
  top: "conv3_bn"
  include: { phase: FORWARDA}
  include: { phase: FORWARDB}
}
layer {
  name: "softmax"
  type: "SpatialSoftmax"
  bottom: "conv3_bn"
  top: "conv3_bn"
  spatial_softmax_param {
    engine: CAFFE
  }
  include: { phase: FORWARDA}
  include: { phase: FORWARDB}
}
layer {
  name: "fc_images"
  type: "InnerProduct"
  bottom: "conv3_bn"
  top: "expected_xy"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2  # dimensionality will actually be 2*num_channels
    axis: -2
    weight_filler {
      type: "expectation"
      expectation_option: "xy"
      width: 109
      height: 109
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  include: { phase: FORWARDA}
  include: { phase: FORWARDB}
}
layer {
  name: "flatten_fp"
  type: "Flatten"
  bottom: "expected_xy"
  top: "flat_xy"
  include: {phase:FORWARDA}
  include: { phase: FORWARDB}
}
layer {
  name: "flatten_as"
  type: "Flatten"
  bottom: "all_states"
  top: "flat_all_states"
  include: {phase:FORWARDA}
  include: { phase: FORWARDB}
}
layer {
  name: "concat_features"
  type: "Concat"
  bottom: "flat_all_states"
  bottom: "flat_xy"
  top: "all_cost_features"
  concat_param {
    axis: 1
  }
  include: {phase:FORWARDA}
  include: { phase: FORWARDB}
}
layer {
  name: "reshape_fp" #input is T*N x 2 x 32, want it to be NxTx64
  type: "Reshape"
  bottom: "all_cost_features"
  top: "cost_features"
  reshape_param {
    shape {
      dim: 1 # FORWARDB is always a single sample
      dim: 25
      dim: 96 # 32+64
      dim: 1
    }
  }
  include: { phase: FORWARDB}
}
## END: FORWARDA NETWORK ##

## START: TEST NETWORK ##
# 2 layer neural net
layer {
  name: "fc_enc1"
  type: "InnerProduct"
  bottom: "cost_features"
  top: "enc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  include: {phase:TEST}
  include: {phase:TRAIN}
  include: { phase: FORWARDB}
}
layer {
  name: "relu1b"
  type: "ReLU"
  bottom: "enc1"
  top: "enc1"
  include: {phase:TEST}
  include: {phase:TRAIN}
  include: { phase: FORWARDB}
}
layer {
  name: "fc_enc2"
  type: "InnerProduct"
  bottom: "enc1"
  top: "enc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  include: {phase:TEST}
  include: {phase:TRAIN}
  include: { phase: FORWARDB}
}
layer {
  name: "relu2b"
  type: "ReLU"
  bottom: "enc2"
  top: "enc2"
  include: {phase:TEST}
  include: {phase:TRAIN}
  include: { phase: FORWARDB}
}
layer {
  name: "fc_enc3"
  type: "InnerProduct"
  bottom: "enc2"
  top: "enc_all"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  include: {phase:TEST}
  include: {phase:TRAIN}
  include: { phase: FORWARDB}
}
# Affine cost layer
layer {
  name: "fc1_affine"
  type: "InnerProduct"
  bottom: "enc_all"
  top: "Ax"
  param {
    lr_mult: 1 # 1 for full cold start
    decay_mult: 1
  }
  param {
    lr_mult: 2 # 2 for full cold start
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100 # What should this be??
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  include: {phase:TEST}
  include: {phase: TRAIN}
}
layer {
  name: "dotproduct1"
  type: "Eltwise"
  bottom: "Ax"
  bottom: "Ax"
  top: "AxAx"
  eltwise_param {
    operation: PROD
  }
  include: {phase:TEST}
  include: {phase: TRAIN}
}
layer {
  name: "fc_sum"
  type: "InnerProduct"
  bottom: "AxAx"
  top: "2x_allcost"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1  # Output is 2x cost for every time step - NxT
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  include: {phase:TEST}
  include: {phase:TRAIN}
}
## END: TEST NETWORK ##

## START: EXTRA TRAIN LAYERS ##
layer {
  name: "demo_sample_slice"
  type: "Slice"
  bottom: "2x_allcost"
  top: "demo_cost"
  top: "sample_cost"
  slice_param {
    axis: 0
    slice_point: 5 # Needs to be equal to demo batch size.
  }
  include: {phase:TRAIN}
}
layer {
  name: "loss"
  type: "IOCLoss"
  bottom: "demo_cost"
  bottom: "sample_cost"
  bottom: "demo_logis"
  bottom: "sample_logis"
  top: "ioc_loss"
  loss_weight: 1 # Used 1 for demo batch size of 10.
  include: {phase:TRAIN}
}
# Regularization Loss
## Make an input of all zeros
layer {
  name: "zeros"
  type: "Power"
  bottom: "Ax"
  top: "zeros"
  power_param {
    scale: 0
    power: 1
    shift: 0
  }
  include: {phase:TRAIN}
}
layer {
  name: "reg_loss"
  type: "EuclideanLoss"
  bottom: "Ax"
  bottom: "zeros"
  top: "reg_loss"
  loss_weight: 5e-4
  include: {phase:TRAIN}
}
## END: EXTRA TRAIN LAYERS ##

