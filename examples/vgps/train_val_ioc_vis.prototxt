name: "JointAndImageStateRegressionNet"
layer {
  name: "image_data"
  type: "ImageData"
  top: "demo_images"
  image_data_param {
    source: "img_database_temp/demo_ioc_images.txt"
    batch_size: 20*4
  }
}
layer {
  name: "image_data"
  type: "ImageData"
  top: "sample_images"
  image_data_param {
    source: "img_database_temp/demo_ioc_images.txt"
    batch_size: 20*4
  }
}

layer {
  # Input is NxTxF
  name: "data_in_demos"
  type: "MemoryData"
  top: "demo_cost_phi"
  top: "demo_logis"
  memory_data_param {
    input_shapes {
      dim: 4  # batch size, must be same as slice point
      dim: 20  # T
      dim: 32  # 64+32, dimension of phi
      dim: 1
    }
    input_shapes {
      dim: 4  # batch size, must be same as slice point
      dim: 1
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "data_in_samples"
  type: "MemoryData"
  top: "sample_cost_phi"
  top: "sample_logis"
  memory_data_param {
    input_shapes {
      dim: 4  # batch size, must be same as slice point
      dim: 20  # T
      dim: 32  # 64+32, dimension of phi
      dim: 1
    }
    input_shapes {
      dim: 4  # batch size, must be same as slice point
      dim: 1
      dim: 1
      dim: 1
     }
  }
  include: {phase: TRAIN}
}
layer {
  name: "concat_states"
  type: "Concat"
  bottom: "demo_cost_phi"
  bottom: "sample_cost_phi"
  top: "cost_features"
  concat_param {
    axis: 0
  }
}
layer {
  name: "concat_states"
  type: "Concat"
  bottom: "demo_images"
  bottom: "sample_images"
  top: "all_images"
  concat_param {
    axis: 0
  }
}

# Image Processing Layers
layer {
  name: "conv1/7x7_s2"
  type: "Convolution"
  bottom: "all_images"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  include: { phase: FORWARDA}
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
  include: { phase: FORWARDA}
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  include: { phase: FORWARDA}
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
  include: { phase: FORWARDA}
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "conv2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 2
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  include: { phase: FORWARDA}
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
  include: { phase: FORWARDA}
}
layer {
  name: "softmax"
  type: "SpatialSoftmax"
  bottom: "conv3"
  top: "conv3"
  spatial_softmax_param {
    engine: CAFFE
    temperature: 1.0
  }
}
layer {
  name: "fc_images"
  type: "InnerProduct"
  bottom: "conv3"
  top: "expected_xy"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2  # dimensionality will actually be 2*num_channels
    axis: -2
    weight_filler {
      type: "expectation"
      expectation_option: "xy"
      width: 109
      height: 109
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "reshape_fp" input is T*N x 2 x 32, want it to be NxTx64
  type: "Reshape"
  bottom: "expected_xy"
  top: "reshape_xy"
  reshape_param {
    shape {
      dim: 4
      dim: 20
      dim: 64
      dim: 1
    }
  }
}
#layer {
#  name: "reshape_as"
#  type: "Reshape"
#  bottom: "all_states"
#  top: "flat_all_states"
#}
layer {
  name: "concat_features"
  type: "Concat"
  bottom: "all_states"
  bottom: "reshape_xy"
  top: "all_cost_features"
  concat_param {
    axis: 2
  }
}

# Affine cost layer
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "cost_features"
  top: "Ax"
  param {
    lr_mult: 1 # 1 for full cold start
    decay_mult: 1
  }
  param {
    lr_mult: 2 # 2 for full cold start
    decay_mult: 0
  }
  inner_product_param {
    num_output: 50 # What should this be??
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "dotproduct1"
  type: "Eltwise"
  bottom: "Ax"
  bottom: "Ax"
  top: "AxAx"
  eltwise_param {
    operation: PROD
  }
}
layer {
  name: "fc_sum"
  type: "InnerProduct"
  bottom: "AxAx"
  top: "2x_allcost"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1  # Output is 2x cost for every time step - NxT
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

layer {
  name: "demo_sample_slice"
  type: "Slice"
  bottom: "2x_allcost"
  top: "demo_cost"
  top: "sample_cost"
  slice_param {
    axis: 0
    slice_point: 4 # Needs to be equal to demo batch size.
  }
}
layer {
  name: "loss"
  type: "IOCLoss"
  bottom: "demo_cost"
  bottom: "sample_cost"
  bottom: "demo_logis"
  bottom: "sample_logis"
  top: "ioc_loss"
  loss_weight: 1 # Used 1 for demo batch size of 10.
}
# Regularization Loss
## Make an input of all zeros
layer {
  name: "zeros"
  type: "Power"
  bottom: "Ax"
  top: "zeros"
  power_param {
    scale: 0
    power: 1
    shift: 0
  }
}
layer {
  name: "reg_loss"
  type: "EuclideanLoss"
  bottom: "Ax"
  bottom: "zeros"
  top: "reg_loss"
  loss_weight: 1e-4
  include: {phase:TRAIN}
}

