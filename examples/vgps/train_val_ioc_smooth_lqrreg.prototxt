name: "JointAndImageStateRegressionNet"
layer {
  # Input is NxTxF
  name: "data_in_demos"
  type: "MemoryData"
  top: "demos"
  top: "dlogis"
  top: "dlqr_cost"
  memory_data_param {
    input_shapes {
      dim: 5  # batch size, must be same as slice point
      dim: 100  # T
      dim: 26  # dimension of phi
      dim: 1
    }
    input_shapes {
      dim: 5  # batch size, must be same as slice point
      dim: 1
      dim: 1
      dim: 1
    }
    input_shapes {
      dim: 5  # batch size, must be same as slice point
      dim: 100  # T
      dim: 1
      dim: 1
    }
  }
  include: {phase: TRAIN}
}
layer {
  name: "data_in_samples"
  type: "MemoryData"
  top: "samples"
  top: "slogis"
  top: "slqr_cost"
  memory_data_param {
    input_shapes {
      dim: 5  # batch size, must be same as slice point
      dim: 100  # T
      dim: 26  # dimension of phi
      dim: 1
    }
    input_shapes {
      dim: 5  # batch size, must be same as slice point
      dim: 1
      dim: 1
      dim: 1
    }
    input_shapes {
      dim: 5 # batch size
      dim: 100 # T
      dim: 1
      dim: 1
    }
  }
  include: {phase: TRAIN}
}

layer {
  name: "demo_sample_concat"
  type: "Concat"
  bottom: "demos"
  bottom: "samples"
  top: "all_traj"
  concat_param {
    axis: 0
  }
}
layer {
  name: "demo_sample_lqr_concat"
  type: "Concat"
  bottom: "dlqr_cost"
  bottom: "slqr_cost"
  top: "all_lqr_cost"
  concat_param {
    axis: 0
  }
}
layer {
  name: "fc_enc1"
  type: "InnerProduct"
  bottom: "all_traj"
  top: "enc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 52
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "identity"
      identity_option: "both"
      #type: "gaussian"
      #std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "enc1"
  top: "enc1"
}

layer {
  name: "fc_enc2"
  type: "InnerProduct"
  bottom: "enc1"
  top: "enc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 52
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "identity"
      identity_option: "positive"
      #type: "gaussian"
      #std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "enc2"
  top: "enc2"
}
layer {
  name: "fc_enc3"
  type: "InnerProduct"
  bottom: "enc2"
  top: "enc_all"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}


layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "enc_all"
  top: "Ax"
  param {
    lr_mult: 1 # 1 for full cold start
    decay_mult: 1
  }
  param {
    lr_mult: 2 # 2 for full cold start
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

layer {
  name: "dotproduct1"
  type: "Eltwise"
  bottom: "Ax"
  bottom: "Ax"
  top: "AxAx"
  eltwise_param {
    operation: PROD
  }
}

layer {
  name: "fc_sum"
  type: "InnerProduct"
  bottom: "AxAx"
  top: "all_cost"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1  # Output is cost for every time step - NxT
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "demo_sample_slice"
  type: "Slice"
  bottom: "all_cost"
  top: "demo_cost"
  top: "sample_cost"
  slice_param {
    axis: 0
    slice_point: 5 # Needs to be equal to demo batch size.
  }
}

# Slowness loss handling, input NxTx1
layer {
  name: "demo-1_slice"
  type: "Slice"
  bottom: "all_cost"
  top: "t-1_cost"
  top: "end_cost"
  slice_param {
    axis: 1
    slice_point: 98
  }
}
layer {
  name: "demo+1_slice"
  type: "Slice"
  bottom: "all_cost"
  top: "start_cost"
  top: "t+1_cost"
  slice_param {
    axis: 1
    slice_point: 2
  }
}
layer {
  name: "demo+0_slice"
  type: "Slice"
  bottom: "all_cost"
  top: "start_cost1"
  top: "t+0_cost"
  top: "end_cost1"
  slice_param {
    axis: 1
    slice_point: 1
    slice_point: 99
  }
}

layer {
  name: "demo_t+1-demo_t"
  type: "Eltwise"
  bottom: "t+1_cost"
  bottom: "t+0_cost"
  top: "cost_diff1"
  eltwise_param {
    operation: SUM
    coeff: 1
    coeff: -1
  }
}
layer {
  name: "demo_t-demo_t-1"
  type: "Eltwise"
  bottom: "t+0_cost"
  bottom: "t-1_cost"
  top: "cost_diff-1"
  eltwise_param {
    operation: SUM
    coeff: 1
    coeff: -1
  }
}

layer {
  name: "slow_l2_reg_loss"
  type: "EuclideanLoss"
  bottom: "cost_diff1"
  bottom: "cost_diff-1"
  top: "slow_l2_loss"
  loss_weight: 0.1 # 0.1
}
layer {
  name: "demo_sample_diffslice"
  type: "Slice"
  bottom: "cost_diff1"
  top: "demo_costdiff"
  top: "sample_costdiff"
  slice_param {
    axis: 0
    slice_point: 5 # Needs to be equal to demo batch size.
  }
}
layer {
  name: "reshape_costdiff"
  type: "Reshape"
  bottom: "demo_costdiff"
  top: "demo_costdiff_reshape"
  reshape_param {
    shape {
      dim: -1
      dim: 1
      dim: 0
    }
  }
}
layer {
  name: "silence_layer"
  type: "Silence"
  bottom: "start_cost"
  bottom: "end_cost"
  bottom: "start_cost1"
  bottom: "end_cost1"
  bottom: "sample_costdiff"
}
layer {
  name: "monotonic_loss"
  type: "HingeLoss"
  bottom: "demo_costdiff_reshape"
  top: "monotonic_loss"
  hinge_loss_param {
    norm: L2
  }
  loss_weight: 100.0 #0.0000000001
}

# Regularization Loss
layer {
  name: "fc_lqr_enc"
  type: "InnerProduct"
  bottom: "all_lqr_cost"
  top: "all_lqr_plus"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "identity"
      identity_option: "positive"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "demo_lqr_reg"
  type: "EuclideanLoss"
  bottom: "all_cost"
  bottom: "all_lqr_plus"
  top: "all_lqrreg_cost"
  loss_weight: 1e-7
}


#layer {
#  name: "cost_lqr_mult"
#  type: "Eltwise"
#  bottom: "slqr_ecost_reshape"
#  bottom: "sample_cost"
#  top: "weighted_sample_cost"
#  eltwise_param {
#    operation: PROD
#  }
#}
#layer {
#  name: "lqr_samplecost_reg"
#  type: "HingeLoss"
#  bottom: "weighted_sample_cost"
#  top: "lqr_reg_loss"
#  hinge_loss_param {
#    norm: L2
#  }
#  loss_weight: 0.001
#}

layer {
  name: "loss"
  type: "IOCLoss"
  bottom: "demo_cost"
  bottom: "sample_cost"
  bottom: "dlogis"
  bottom: "slogis"
  top: "ioc_loss"
  loss_weight: 1.0 # Used 1 for most things, needs to be more for subsampled
}
