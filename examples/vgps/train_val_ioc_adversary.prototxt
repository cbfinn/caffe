name: "JointAndImageStateRegressionNet"
layer {
  # Input is NxTxF
  name: "data_in_demos"
  type: "MemoryData"
  top: "demos"
  top: "dlogis"
  memory_data_param {
    input_shapes {
      dim: 5  # batch size, must be same as slice point
      dim: 100  # T
      dim: 26  # dimension of phi
      dim: 1
    }
    input_shapes {
      dim: 5  # batch size, must be same as slice point
      dim: 1
      dim: 1
      dim: 1
    }
  }
  include: {phase: TRAIN}
}
layer {
  name: "data_in_samples"
  type: "MemoryData"
  top: "samples"
  top: "slogis"
  memory_data_param {
    input_shapes {
      dim: 5  # batch size, must be same as slice point
      dim: 100  # T
      dim: 26  # dimension of phi
      dim: 1
    }
    input_shapes {
      dim: 5  # batch size, must be same as slice point
      dim: 1
      dim: 1
      dim: 1
    }
  }
  include: {phase: TRAIN}
}

layer {
  name: "demo_sample_concat"
  type: "Concat"
  bottom: "demos"
  bottom: "samples"
  top: "all_traj"
  concat_param {
    axis: 0
  }
}

layer {
  name: "fc_enc1"
  type: "InnerProduct"
  bottom: "all_traj"
  top: "enc1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "enc1"
  top: "enc1"
}
layer {
  name: "fc_enc2"
  type: "InnerProduct"
  bottom: "enc1"
  top: "enc2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "enc2"
  top: "enc2"
}
layer {
  name: "fc_enc3"
  type: "InnerProduct"
  bottom: "enc2"
  top: "enc_all"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}


layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "enc_all"
  top: "Ax"
  param {
    lr_mult: 1 # 1 for full cold start
    decay_mult: 0
  }
  param {
    lr_mult: 2 # 2 for full cold start
    decay_mult: 0
  }
  inner_product_param {
    num_output: 100
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

layer {
  name: "dotproduct1"
  type: "Eltwise"
  bottom: "Ax"
  bottom: "Ax"
  top: "AxAx"
  eltwise_param {
    operation: PROD
  }
}

layer {
  name: "fc_sum"
  type: "InnerProduct"
  bottom: "AxAx"
  top: "all_cost"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1  # Output is cost for every time step - NxT
    axis: 2  # Input is NxTxF
    weight_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "demo_sample_slice"
  type: "Slice"
  bottom: "all_cost"
  top: "demo_cost"
  top: "sample_cost"
  slice_param {
    axis: 0
    slice_point: 5 # Needs to be equal to demo batch size.
  }
}

# Regularization Loss

## Make an input of all zeros
layer {
  name: "zeros"
  type: "Power"
  bottom: "sample_cost"
  top: "zeros"
  power_param {
    scale: 0
    power: 1
    shift: 0
  }
}
layer {
  name: "reg_loss"
  type: "EuclideanLoss"
  bottom: "sample_cost"
  bottom: "zeros"
  top: "reg_loss"
  loss_weight: 1e-3
}

layer {
  name: "loss"
  type: "IOCLoss"
  bottom: "demo_cost"
  bottom: "sample_cost"
  bottom: "dlogis"
  bottom: "slogis"
  top: "ioc_loss"
  loss_weight: 1 # Used 1 for most things, needs to be more for subsampled
}


### Domiain label handling ###
layer {
  name: "source_domain_label"
  type: "DummyData"
  top: "source_domain_label"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "constant"
      value: 0
    }
    shape {
      dim: 500 # 100*5
      dim: 1
    }
  }
}
layer {
  name: "target_domain_label"
  type: "DummyData"
  top: "target_domain_label"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "constant"
      value: 1
    }
    shape {
      dim: 500 # 100*5
      dim: 1
    }
  }
}
layer {
  name: "domain_label"
  type: "Concat"
  bottom: "source_domain_label"
  bottom: "target_domain_label"
  top: "domain_label"
  concat_param {
    concat_dim: 0
  }
  include { phase: TRAIN }
}
layer {
  name: "inverted_domain_label"
  type: "Concat"
  bottom: "target_domain_label"
  bottom: "source_domain_label"
  top: "inverted_domain_label"
  concat_param {
    concat_dim: 0
  }
  include { phase: TRAIN }
}

#### Start of confusion layers ####
layer {
  name: "reshapeenc"
  bottom: "enc_all"
  top: "enc_reshape"
  type: "Reshape"
  reshape_param {
    shape {
      dim: 1000
      dim: 100
    }
  }
}
layer {
  name: "domain_classifier"
  type: "InnerProduct"
  bottom: "enc_reshape"
  top: "domain"
  param {
    name: "domain_w"
    lr_mult: 4.0
    decay_mult: 0
  }
  param {
    name: "domain_b"
    lr_mult: 8.0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
  include { phase: TRAIN }
}
layer {
  name: "domain_loss"
  type: "SoftmaxWithLoss"
  bottom: "domain"
  bottom: "domain_label"
  top: "domain_loss"
  loss_weight: 1.0
  include { phase: TRAIN }
}
layer {
  name: "domain_acc"
  type: "Accuracy"
  bottom: "domain"
  bottom: "domain_label"
  top: "domain_acc"
  include {
    phase: TRAIN
  }
}
layer {
  name: "inverted_domain_classifier"
  type: "InnerProduct"
  bottom: "enc_reshape"
  top: "inverted_domain"
  param {
    name: "domain_w"
    lr_mult: 0
    decay_mult: 0
  }
  param {
    name: "domain_b"
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
  include { phase: TRAIN }
}
layer {
  name: "inverted_domain_loss"
  type: "SoftmaxWithLoss"
  bottom: "inverted_domain"
  bottom: "inverted_domain_label"
  top: "inverted_domain_loss"
  loss_weight: 1.0
  include { phase: TRAIN }
}

